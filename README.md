# Excimontec
The goal of this project is to develop an open-source KMC simulation software package for modeling organic semiconductor materials and devices, such as OPVs, OLEDs, and more. 
The software is being developed in modern C++ and is optimized for efficient execution on high performance computing clusters using MPI. 
This software package uses object-oriented design and extends the [KMC_Lattice](https://github.com/MikeHeiber/KMC_Lattice) framework.

#### Major Features:
- Adjustable periodic boundary conditions in all three directions allow users to perform 1D, 2D, or 3D simulations.
- Choose between several film architectures, including a neat film, bilayer film, or random blend film.
- Import bulk heterojunction morphologies generated by [Ising_OPV v3.2 and v4](https://github.com/MikeHeiber/Ising_OPV).
- Donor and acceptor materials can take on an uncorrelated Gaussian DOS, a correlated Gaussian DOS with different correlation functions, or an uncorrelated exponential DOS model.
- Dynamics test simulations can be performed to generate exciton and charge carrier density transients that can be used to model exciton dissociation, charge carrier separation, and charge carrier recombination kinetics.
- Time-of-flight charge transport simulations of electrons or holes can be performed on neat, random blend, or bulk heterojunction blend films.
- Exciton diffusion simulations can be performed on any film architecture.
- Internal quantum efficiency simulations can be performed on bilayer, random blend, or bulk heterojunction blend films.
- Simulate complex exciton dynamics with events for intersystem crossing between singlet and triplet states as well as exciton-exciton and exciton-polaron annihilation events.
- Choose between Miller-Abrahams or Marcus models for polaron hopping.
- Charge carrier delocalization can be modeled with a spherical Gaussian delocalization model.
- Choose between several KMC algorithms (first reaction method, selective recalculation method, or full recalculation method).

## Current Status
The current version (Excimontec v1.0-beta.3) is built with KMC_Lattice v2.0-beta.3 and allows the user to perform several simulation tests relevant for OPV and OLED devices. 
All major planned features that are to be included in v1.0 are now implemented and have undergone preliminary testing. 
However, this software tool is still under development, and as such, there may still be bugs that need to be squashed. 
Please report any bugs or submit feature requests in the [Issues](https://github.com/MikeHeiber/Excimontec/issues) section. 

Major releases and other significant developments will be announced on the Excimontec: General News mailing list. If you are interested in keeping up to date with the development and application of this tool, please subscribe at the following link:
[Subscribe Here](http://eepurl.com/dis9AT)

Travis CI continuous integration status:

Master branch: [![Build Status](https://travis-ci.org/MikeHeiber/Excimontec.svg?branch=master)](https://travis-ci.org/MikeHeiber/Excimontec)

Development branch: [![Build Status](https://travis-ci.org/MikeHeiber/Excimontec.svg?branch=development)](https://travis-ci.org/MikeHeiber/Excimontec)

## Work Together

If you would like to contribute to the development of this project or would like some help in using the tool for your research, please contact me (heiber@mailaps.org) to discuss a collaboration. 
You can check out my KMC research and other work on [Researchgate](https://www.researchgate.net/profile/Michael_Heiber).

Have a quick question or want to chat about Excimontec?  Join the dicussion on Gitter: [![Gitter chat](https://badges.gitter.im/Excimontec.png)](https://gitter.im/Excimontec)

## How to try Excimontec?

#### Building an Executable

This software tool uses Message Passing Interface (MPI) to utilize parallel computing power. 
As a result, using Excimontec requires that an MPI library is pre-installed on your system, and the final Excitmontec executable must be built on your specific system. 
We cannot provide pre-built binaries for your system. 
Contact your HPC admin to determine the protocols for building MPI applications on your HPC system. 
In many cases, the HPC system will already be configured for you, and the package comes with a default makefile that can be used with the gcc compiler. 

If you wish, you can also install MPI on your own personal workstation and then build Excimontec there as well. For development and preliminary simulation tests, sometimes it is more efficient to run on your own workstation instead of an HPC system. More information about common MPI packages can be found here:
- http://www.open-mpi.org/
- http://www.mpich.org/
- http://mvapich.cse.ohio-state.edu/

#### Usage
In most cases, your HPC system will use a job scheduler to manage the computing workload. 
For performing Excimontec simulations, it is recommended to submit batch jobs where you will request the resources needed to perform the simulation. 
An example batch script for the SLURM job scheduling system is provided with this package (slurm_script.sh). 
Similar batch scripts can also be written for TORQUE or other job schedulers.

Regardless of the job scheduler, the program execution command is essentially the same. 
Excimontec.exe takes one required input argument, which is the filename of the input parameter file. 
An example parameter file is provided with this package (parameters_default.txt).

For example, within the batch script, to create a simulation that runs on 10 processors, an the execution command is:
>    mpiexec -n 10 Excimontec.exe parameters_default.txt

In this example, the parameters_default.txt file that is located in the current working directory is loaded into the Excimontec program to determine what simulation to run.

#### Output
Excimontec will create a number of different output files depending which test is chosen in the parameter file:
- results#.txt -- This text file will contain the results for each processor where the # will be replaced by the processor ID.
- analysis_summary.txt -- When MPI is enabled, this text file will contain average final results from all of the processors.
- dynamics_average_transients.txt -- When performing a dynamics test, calculated exciton, electron, and hole transients will be output to this file.
- ToF_average_transients.txt -- When performing a time-of-flight charge transport test, calculated current transients, mobility relaxation transients, and energy relaxation transients will be output to this file.
- ToF_transit_time_dist.txt -- When performing a time-of-flight charge transport test, the resulting polaron transit time probability distribution will be output to this file.
- ToF_results.txt -- When performing a time-of-flight charge transport test, the resulting quantitative results are put into this parsable delimited results file.
- Charge_extraction_map#.txt -- When performing a time-of-flight or IQE test, the x-y locations where charges are extracted from the lattice are saving into this map file.

#### Data Analysis
For [Igor Pro](https://www.wavemetrics.com/) users, I am developing an open-source procedures package for loading, analyzing, and plotting data from Excimontec simulations called [Excimontec_Analysis](https://github.com/MikeHeiber/Excimontec_Analysis). 
This is a good starting point for managing the data generated by Excimontec, and the Igor Pro scripting environment provides a nice playground where users can perform more advanced data analysis as needed.

## For Software Developers
Public API documentation for the Excimontec package is still under development and can be viewed [here](https://mikeheiber.github.io/Excimontec/).
